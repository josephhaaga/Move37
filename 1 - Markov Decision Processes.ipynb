{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reinforcement Learning** is a machine learning technique exhibiting characteristics of both supervised and unsupervised learning. Labels (a.k.a _rewards_ or _y_ outputs) are sparse and time-delayed, and the consequences of an agent's actions are not guaranteed to be immediately observable. Current solutions involve the combination of pattern recognition networks, and realtime environment learning frameworks (deep reinforcement learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "An online product delivery startup provides logistical analytics to determine the best delivery route for a product. This dynamic problem is an ideal candidate for reinforcement learning, requiring a learning system that is adaptive to changes. Furthermore, there is no existing dataset to learn from, and learning must be done in _real time_ (time bering a new dimension when compared to traditional supervised learning). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain\n",
    "A system for modelling a chain of linked events. Includes a set of states and a process for moving from one state to another. Each state is a single step, and is based on a _transition matrix_ **T** of probabilities. The next state, _s<sub>t+1</sub>_ is based purely on the existing state. \n",
    "\n",
    "### Base Case: ###\n",
    "**S<sub>0</sub>** = (1 0 0)\n",
    "\n",
    "### Inductive Step: ###\n",
    "**S<sub>t+1</sub>** = **S<sub>t</sub>** * **T**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process ##\n",
    "An extension of Markov Chains, introducting agent **actions** and **rewards** as a result of those actions.\n",
    "\n",
    "### Components: ###\n",
    "1. Set of possible states **S**\n",
    "2. An initial state **S<sub>t=0</sub>**\n",
    "    - Starting State Distribution is \\beta\n",
    "3. Set of possible actions **A**\n",
    "4. Transition model **Pr(s'|a,s)**\n",
    "5. Reward function **r(s,a)**\n",
    "    - returns a real value every time agent moves from one state to another\n",
    "    \n",
    "### Goal ###\n",
    "Find a **policy** that selects an action with the highest **reward**. This _optimal_ policy provides the greatest utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman Equation\n",
    "## Finding the Optimal Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
