{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reinforcement Learning** is a machine learning technique exhibiting characteristics of both supervised and unsupervised learning. Labels (a.k.a _rewards_ or _y_ outputs) are sparse and time-delayed, and the consequences of an agent's actions are not guaranteed to be immediately observable. Current solutions involve the combination of pattern recognition networks, and realtime environment learning frameworks (deep reinforcement learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "An online product delivery startup provides logistical analytics to determine the best delivery route for a product. This dynamic problem is an ideal candidate for reinforcement learning, requiring a learning system that is adaptive to changes. Furthermore, there is no existing dataset to learn from, and learning must be done in _real time_ (time bering a new dimension when compared to traditional supervised learning). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain\n",
    "A system for modelling a chain of linked events. Includes a set of states and a process for moving from one state to another. Each state is a single step, and is based on a _transition matrix_ **T** of probabilities. The next state, _s<sub>t+1</sub>_ is based purely on the existing state. \n",
    "\n",
    "### Base Case: ###\n",
    "**S<sub>0</sub>** = (1 0 0)\n",
    "\n",
    "### Inductive Step: ###\n",
    "**S<sub>t+1</sub>** = **S<sub>t</sub>** * **T**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process ##\n",
    "An extension of Markov Chains, introducting agent **actions** and **rewards** as a result of those actions.\n",
    "\n",
    "### Components: ###\n",
    "1. Set of possible states **S**\n",
    "2. An initial state **S<sub>t=0</sub>**\n",
    "    - Starting State Distribution is \\beta\n",
    "3. Set of possible actions **A**\n",
    "4. Transition model **Pr(s'|a,s)**\n",
    "5. Reward function **r(s,a)**\n",
    "    - returns a real value every time agent moves from one state to another\n",
    "    \n",
    "### Goal ###\n",
    "Find a **policy** that selects an action with the highest **reward**. This _optimal_ policy provides the greatest utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman Equation\n",
    "## Finding the Optimal Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "**State:** A numeric representation of what the agent is observing at a particular point in time in the enviornment.\n",
    "\n",
    "**Action:** The input the agent provides to the environment, calculated by applying a policy to the current state.\n",
    "\n",
    "**Reward:** A feedback signal from the environment reflecting how well the agent is performing the goals of the game (points, kills etc.)\n",
    "\n",
    "\n",
    "**Dynamic Programming:** A class of algorithms that simplify complex problems by breaking them into sub-problems and solving the sub-problems recursively.\n",
    "\n",
    "\n",
    "### Goal\n",
    "\n",
    "Given the current state, choose the optimal action to maximize the long-term expected reward provided by the environment.\n",
    "\n",
    "The Bellman Equation answers the following question:\n",
    "**Given the state I'm in, assuming I take the best possible action nwo and at each subsequent step, what long-term reward can I expect?**\n",
    "\n",
    "i.e. What is the Value of the State?\n",
    "\n",
    "Helps up evaluate the expected reward relative to the [dis]advantage of each state.\n",
    "\n",
    "## Bellman Equation for Determinisic Environments\n",
    "\n",
    "![Bellman Equation for Deterministic environments](./assets/BellmanEqDeterministic.png \"Bellman Equation for Deterministic environments\")\n",
    "\n",
    "**V(s):** Value of State\n",
    "\n",
    "**max<sub>a</sub>:** Action with maximum value\n",
    "\n",
    "**R(s,a):** Reward of action _a_ at state _s_\n",
    "\n",
    "**\\lambda:** discount factor [0.9,0.99] (lower value encourage short-term thinking, whereas a higher value emphasizes long-term rewards)\n",
    "\n",
    "**V(s'):** Value of future state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mario Example\n",
    "![Mario Example - Deterministic environment](./assets/MarioExampleDeterministic.png \"Mario Example - Deterministic environment\")\n",
    "\n",
    "Starting from the reward state and working backwards, we plug & chug to determine the state value of each square. \n",
    "\n",
    "Notice the distinction between Value and Reward (all the zeroes in the non-terminal states), and the underlying assumption that the agent will take the action with the _highest transition probability._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
